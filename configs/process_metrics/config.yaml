extensions:
  pic_control:
    policy_file_path: /etc/sa-omf/process_metrics/policy.yaml
    max_patches_per_minute: 5
    patch_cooldown_seconds: 5
    auto_apply_patches: true  # Enable automatic application of patches

receivers:
  # Process metrics collection - exclusively focus on process scraper
  hostmetrics:
    collection_interval: 15s  # Slightly longer interval for reduced overhead
    scrapers:
      process:
        include:
          match_type: regexp
          processes: [".*"]  # Collect metrics for all processes
        metrics:
          # Explicitly list only the needed process metrics to minimize overhead
          process.cpu.time: true
          process.cpu.utilization: true
          process.memory.rss: true
          process.memory.virtual: true
          process.disk.io: true
          process.disk.operations: true
          process.threads: true
          process.open_file_descriptors: true
          process.context_switches: false  # Disable high-volume metrics
          # Disable other metrics we don't need for the process metrics model
          process.signals: false
          process.pagefaults: false
        resource_attributes:
          # Control which resource attributes to collect
          process.executable.name: true   # Essential
          process.executable.path: false  # Not needed
          process.command: false          # Not needed
          process.command_line: false     # Disabled as processor later removes this
          process.pid: false              # Disabled since processor deletes this attribute
          process.parent_pid: false       # Not needed
          process.owner: true             # Useful for grouping
          process.username: false         # Not needed
  
  # Self-metrics collection
  prometheus/self:
    config:
      scrape_configs:
        - job_name: 'phoenix-process-metrics'
          scrape_interval: 5s
          static_configs:
            - targets: ['localhost:8888']
          metrics_path: '/metrics'

processors:
  # Convert cumulative process.cpu.time to delta for better visualization
  cumulativetodelta:
    include:
      match_type: strict
      metrics:
        - process.cpu.time
        - process.io.read_bytes
        - process.io.write_bytes
    max_stale: 15s
  
  # Batch metrics for efficient export to New Relic
  batch:
    send_batch_size: 1000
    send_batch_max_size: 2000
    timeout: 5s
  
  # Unified metric_pipeline processor for process-metrics
  metric_pipeline:
    # Resource filtering configuration 
    resource_filter:
      enabled: true
      filter_strategy: hybrid
      priority_attribute: "aemf.process.priority"
      priority_rules:
        - match: "process.executable.name=~/java|javaw/"
          priority: high
        - match: "process.executable.name=~/nginx|httpd|apache2/"
          priority: high
        - match: "process.executable.name=~/mysql|postgres|mongod|redis-server|elasticsearch/"
          priority: critical
        - match: "process.executable.name=~/python|python3|node|dotnet|ruby/"
          priority: medium
        - match: "process.executable.name=~/otelcol|sa-omf-otelcol|collector/"
          priority: critical
        - match: "process.executable.name=~/newrelic-infra|nri-.*/"
          priority: critical
        - match: ".*"
          priority: low
      topk:
        k_value: 25   # Start with 25 processes tracked individually
        k_min: 10
        k_max: 50
        resource_field: "process.executable.name"
        counter_field: "process.cpu.time"  # Use CPU time as the ranking field
        coverage_target: 0.95
      rollup:
        enabled: true
        priority_threshold: low
        strategy: sum
        name_prefix: "phoenix.others.process"  # Use a prefix making clear these are aggregated
    
    # Metric transformation configuration tailored for New Relic
    transformation:
      # Generate CPU time histograms for New Relic's histogram visualization features
      histograms:
        enabled: true
        max_buckets: 10
        metrics:
          process.cpu.time:
            boundaries: [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 30.0]
          process.memory.rss:
            boundaries: [1048576, 10485760, 104857600, 524288000, 1073741824, 2147483648]  # 1MB, 10MB, 100MB, 500MB, 1GB, 2GB
      
      # Attribute processing optimized for New Relic
      attributes:
        actions:
          # Remove high-cardinality attributes after resource filtering is complete
          - key: "process.pid"
            action: "delete"
          - key: "process.command_line"
            action: "delete"
          - key: "process.command"
            action: "delete"
          - key: "process.executable.path"
            action: "delete"
          
          # Conditionally remove container.id (if present) to reduce cardinality
          - key: "container.id"
            action: "delete"
          
          # Ensure process.executable.name is standardized
          - key: "process.executable.name"
            action: "update"
            value: {{ if eq .Value.String "unknown" }}unknown_process{{ else }}{{ .Value.String }}{{ end }}
          
          # Add New Relic specific attributes for better querying
          - key: "collector.name"
            action: "insert"
            value: "Phoenix-SA-OMF"
          - key: "service.name"
            action: "insert"
            value: "phoenix-process-metrics"
          - key: "instrumentation.provider"
            action: "insert" 
            value: "phoenix"
          
          # Tag metrics with priority information for better querying in NRQL
          - key: "phoenix.priority"
            action: "insert"
            value: {{ .Resource.Attributes.GetString "aemf.process.priority" "unknown" }}
          
          # Add rollup indicator for analysts to understand aggregated metrics
          - key: "phoenix.isRolledUp"
            action: "insert"
            value: {{ if eq .Resource.Attributes.GetString "process.executable.name" "phoenix.others.process" }}true{{ else }}false{{ end }}

exporters:
  # Detailed logging for development and debugging
  logging:
    verbosity: basic  # Use basic to avoid overwhelming logs
    sampling_initial: 5  # Only log every 5th metric initially
    sampling_thereafter: 100  # Then log every 100th

  # Export to New Relic through OTLP
  otlphttp/newrelic:
    endpoint: "https://otlp.nr-data.net:4318"  # Use HTTP endpoint (4318) for maximum compatibility
    headers:
      api-key: "${env:NEW_RELIC_API_KEY}"
    timeout: 10s
    compression: gzip  # Enable compression to reduce bandwidth usage
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 4
      queue_size: 1000  # Larger queue to handle bursts
    disable_metrics_histograms: false  # Make sure histograms are enabled for New Relic

  # Connector for configuration patching
  pic_connector:
    # PIC connector forwards config patches to pic_control extension

# Create a dual-pipeline system to ensure reliability
service:
  pipelines:
    # Main process metrics pipeline
    metrics/process:
      receivers: [hostmetrics]
      # Process flow: Convert CPU counters to delta → Process with unified metric pipeline → Batch for efficiency
      processors: [cumulativetodelta, metric_pipeline, batch]
      exporters: [logging, otlphttp/newrelic]
    
    # Self-metrics pipeline to export Phoenix's own metrics to New Relic
    # This allows monitoring the performance and behavior of Phoenix itself
    metrics/phoenix_self:
      receivers: [prometheus/self]
      processors: [batch]
      exporters: [otlphttp/newrelic]
    
    # Control pipeline for self-regulation
    control:
      receivers: [prometheus/self]
      processors: [adaptive_pid]
      exporters: [pic_connector]
  
  extensions: [pic_control]

# Telemetry for the collector itself
telemetry:
  metrics:
    address: localhost:8888
  logs:
    level: info